Claude Prompt (Final – CSV 1 only used in training, not in query):

Part 1 – Thesis Context

We are working on our thesis titled “Enhancing Literary Analysis of Rizal’s Novels Through Topic Modeling: A BERT-Based Approach to Thematic Search Implementation for Noli Me Tangere and El Filibusterismo.”

We have the buod (summary) versions of Noli Me Tangere and El Filibusterismo in digital form.

Our main goal is to build a BERT-based thematic exploration and search system.

The system must:

Accept both Tagalog and English queries.

Recognize deep/archaic Tagalog words (handled via CSV 1 during training).

Leverage BERT’s embeddings to naturally connect synonyms and related concepts (e.g., searching “edukasyon” also retrieves chapters about “pag-aaral”).

Return ranked chapters thematically related to the query (e.g., searching “tinola” retrieves Padre Damaso’s chapter).

Generate a short explanatory paragraph for each result.

Work fully offline (no API/internet).

Important: CSV 1 is only used during training to normalize deep → modern → English words. At runtime (Code 2), queries should not rely on CSV 1. Instead, they should be processed by the trained model directly, since it already “learned” the mappings during training.

Training will be done in Google Colab with GPU enabled. Please also recommend a reasonable number of epochs (e.g., 3–5) for efficient training.

Part 2 – CSV Formats (Fixed)

CSV 1 – Deep Tagalog Words Dictionary
Columns (in order):

book_title → either “Noli Me Tangere” or “El Filibusterismo”

deep_word → the old/archaic Tagalog word

modern_word → its modern Filipino equivalent

english_equivalent → its English translation

CSV 2 – Noli & El Fili Corpus
Columns (in order):

book_title → either “Noli Me Tangere” or “El Filibusterismo”

chapter_number → chapter number in the book

chapter_title → title of the chapter (e.g., “Si Padre Damaso”)

chapter_text → the buod (summary) of the chapter

Part 3 – Code Requirements

Code 1 – Training (CSV 1 + CSV 2 Integrated, on Colab GPU)

Provide Python code that:

Loads CSV 1 (archaic words) and builds a mapping for archaic → modern → English.

Loads CSV 2 (chapters).

Normalizes the chapter text using CSV 1 mapping.

Uses BERTopic with a Tagalog-compatible embedding model (e.g., jcblaise/bert-tagalog-base-cased or XLM-R).

Runs efficiently on Google Colab GPU.

Uses a reasonable number of epochs for fine-tuning (recommend 3–5).

Saves the trained BERTopic model for later querying.

Code 2 – Terminal User Testing Script

Provide Python code that:

Loads the trained BERTopic model.

Lets the user type a query in terminal (Tagalog or English).

Processes the query directly through the trained model (no CSV 1 lookup at runtime).

Retrieves and ranks relevant chapters from the trained model.

Generates a short explanatory paragraph under each result explaining the thematic connection.

Works offline only.

Part 4 – Expected Output Format

Please output your response in this format:

🖥️ Code 1 – Training (Deep Words + Corpus, Colab GPU, recommended epochs)

🖥️ Code 2 – Terminal User Testing Script

✅ Final Notes & Tips

Make the codes detailed but practical for undergraduate thesis students with intermediate Python and NLP knowledge.